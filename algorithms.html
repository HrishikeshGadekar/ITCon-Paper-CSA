<!DOCTYPE html>
<html>

    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- <link rel="shortcut icon" type="image/png" href="https://i.postimg.cc/pXYRd64r/CSAIcon.png" /> -->
        <!-- faivcon -->
        <!-- <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">
        <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff"> -->
        <!--  -->

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Bitter:wght@100;400;500;600;700;800;900&display=swap"
            rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link
            href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@100;200;300;400;500;600;700;800&display=swap"
            rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
            rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link
            href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:wght@400;700&family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
            rel="stylesheet">
        <title>Algorithms</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet"
            integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link
            href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600;700&family=Ubuntu+Mono:wght@400;700&display=swap"
            rel="stylesheet">
        <!-- <link rel="stylesheet" type="text/css" href="csa.css"> -->

        <!--  -->
        <!--  -->

        <style>
            html,
            body {
                /* font-family: "Bitter", serif; */
                /* font-family: "Noto Sans", sans-serif; */
                /* font-family: "Libre Baskerville", serif; */
                font-family: "Source Sans Pro", sans-serif;
                /* background-color: #fefae0; */
                min-height: 100vh;
                display: flex;
                flex-direction: column;
            }

            p {
                text-align: justify;
            }

            .home-b {
                font-weight: 700;
            }

            .lead {
                font-weight: 400;
                font-size: 1.1rem;
            }

            .head-title {
                color: antiquewhite;
                background-color: #a52a2a;
                font-family: "IBM Plex Mono", monospace;
                font-weight: 500;
                margin: auto;
                text-align: center;
                border-left: 4px solid gray;
                border-right: 4px solid gray;
                border-radius: 0 0 0.5em 0.5em;
            }

            /* Nvaigation bar */
            .navbar {
                padding: 0.5em 0;
                background-color: aliceblue;
                font-weight: 500;
                margin-bottom: 0.5rem;
                position: sticky !important;
            }

            .nav-item {
                margin: 0 2em;
                color: #a52a2a;
            }

            .navbar-expand-lg .navbar-collapse {
                /* display: flex !important; */
                justify-content: space-around;
                flex-basis: auto;
            }

            /* Main Content */

            /* .Main-content {
  background-color: white;
} */

            .navg .row {
                font-size: 0.9rem;
            }

            dt a {
                text-decoration: none;
                color: #a52a2a;
            }

            .small-headers {
                font-weight: 700;
            }

            .padc {
                padding: 0.75em 1em;
            }

            /* Footer */
            footer {
                margin-top: auto;
            }

            .foot {
                background-color: #573b38;
                justify-content: space-evenly;
                flex-basis: auto;
            }

            .foot .row .col-3 a {
                text-decoration: none;
                color: #bba9a8;
            }

            .foot .row .col-3 {
                text-align: center;
            }

            /* Demo */
            .demoform {
                padding: 2rem;
                border-radius: 2em;
                background-color: #bba9a869;
                width: 90%;
                margin: auto;
            }

            .demoform input[type="submit"] {
                background-color: #a52a2a;
                color: antiquewhite;
                border: none;
            }

            .demoform input[type="submit"]:hover {
                background-color: #573b38;
                color: #bba9a8;
                border: none;
            }

            .form-label {
                font-size: 1.25rem;
            }

            .demopred {
                text-align: center;
            }

            /* Contact */
            .contactd {
                text-align: center;
                margin: auto 0;
            }

            .contactd a {
                text-decoration: none;
                color: black;
                margin: 0;
            }

            .material-symbols-outlined {
                margin: 0;
            }

            .toc {
                width: 60%;
                border-bottom: 1px solid rgba(211, 211, 211, 0.638);
                padding-bottom: 0.2em;
            }

            .toc-item a {
                text-decoration: none;
                color: #573b38;
            }

            .figc {
                color: #573b38;
                font-style: italic;
                font-weight: 700;
                font-size: 0.95rem;
                margin: auto 0;
            }

            .scroll-top {
                width: 75px;
                height: 75px;
                position: fixed;
                bottom: 25px;
                left: 20px;
                display: none;
            }

            .scroll-top i {
                display: inline-block;
                color: #fff;
            }

            ::-webkit-scrollbar {
                -webkit-appearance: none;
                width: 10px;
            }

            ::-webkit-scrollbar-thumb {
                border-radius: 4px;
                background-color: rgba(0, 0, 0, 0.5);
                box-shadow: 0 0 10px rgba(255, 255, 255, 0.5);
            }

            /* Media Queries */
            @media (min-width: 300px) {
                .head-title {
                    font-size: 1rem;
                }

                .demoform {
                    width: 90%;
                }
            }

            @media (min-width: 600px) {
                .head-title {
                    font-size: 1.25rem;
                }

                .demoform {
                    width: 80%;
                }
            }

            @media (min-width: 1200px) {
                .head-title {
                    font-size: 2.5rem;
                }

                .demoform {
                    width: 70%;
                }
            }
        </style>


        <!--  -->
        <!--  -->



    </head>


    <body>
        <!-- <header> -->
        <div class="container">
            <h1 class="head-title display-8 shadow p-3 mb-15">Machine Learning based automated
                classification of
                worker-reported safety reports in
                construction</h1>
        </div>

        <nav class="navbar navbar-expand-lg navbar-toggler sticky-top">

            <div class="container sticky-top">

                <button class="navbar-toggler sticky-top" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false"
                    aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon sticky-top"></span>
                </button>
                <div class="collapse navbar-collapse sticky-top" id="navbarNavAltMarkup">
                    <div class="navbar-nav center-text sticky-top">
                        <a class="nav-item nav-link" href="index">Home</a>
                        <a class="nav-item nav-link active" href="algorithms">Algorithms</a>
                        <a class="nav-item nav-link" href="demo">Demo</a>
                        <a class="nav-item nav-link" href="contact">Contact</a>
                    </div>
                </div>
            </div>
        </nav>
        <!-- </header> -->

        <!-- Main -->
        <div class="container">
            <p class="lead">
                In this study, experiments were conducted to identify the best performing classifier for the data used
                in the current
                study. For
                any ML algorithm, the parameters known as <b>Hyperparameters</b> control its learning process. The
                prediction
                performance for a given classifier can show significant sensitivity to the value of hyperparameters
                chosen.
                Therefore, in the current study, the value of these hyperparameters is selected by referring to the
                previous ML
                applications on textual data. All the Algorithms were
                implemented using the Python programming language. The adequacy of the hyperparameters is also confirmed
                using a
                <b>Sensitivity analysis</b> between parameter value and performance for a classifier. Information on
                hyperparameters
                for each of the classifiers and detailed results on the sensitivity of prediction
                performance on the algorithm's hyperparameters have been discussed in detail in this section.
            </p>


            <h4 class="sh toc">Table of Contents</h4>
            <ol class="toc-ul">
                <li class="toc-item"><a href="#svm">Lagrangian Support Vector Machines (LSVM)</a></li>
                <li class="toc-item"><a href="#lr">Logistic Regression (LR)</a></li>
                <li class="toc-item"><a href="#bnb">Binomial Naive Bayes (BNB)</a></li>
                <li class="toc-item"><a href="#mnb">Multinomial Naive Bayes (MNB)</a></li>
                <li class="toc-item"><a href="#rf">Random Forest (RF)</a></li>
                <li class="toc-item"><a href="#dt">Decision Trees (DT)</a></li>
                <li class="toc-item"><a href="#cnn">Convolutional Neural Network (CNN)</a></li>
            </ol>

            <hr id="svm">
            <!--1. SVM -->
            <div class="algo">
                <h3 class="sh"><b>1. Lagrangian Support Vector Machines (LSVM)</b></h3>
                <div class="row">
                    <div class="col-md-8">
                        <p>
                            An SVM model is a representation of different classes in a hyperplane in a multidimensional
                            space. The goal of the SVM algorithm is to create the best line or decision boundary that
                            can segregate n-dimensional space into classes so that we can easily put the new data point
                            in the correct category in the future. This best decision boundary is called a hyperplane.
                            The
                            hyperplane will be generated iteratively by SVM so that the error can be minimized.
                            SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme
                            cases are called as support vectors, and hence algorithm is termed as Support Vector
                            Machine.
                        </p>
                        <p>
                            Nonlinear
                            classification can
                            also be done using the kernel trick. Different kernels such as Gaussian kernel, Radial basis
                            function (RBF) kernel,
                            polynomial kernel, etc., are available, but the time complexity is very high compared to the
                            linear
                            model. Also, it
                            is observed that though the RBF kernel, which is a highly complex model, gave good training
                            accuracies, the
                            testing accuracies are comparable to that of the linear model. Hence, due to the high time
                            complexity involved,
                            the linear kernel is used for further analysis. Linear SVM is used for linearly separable
                            data, which means if a dataset can be classified into two classes by using a single straight
                            line, then such data is termed as linearly separable data, and classifier is used called as
                            Linear SVM classifier.
                        </p>l
                    </div>
                    <div class="col-md-4">
                        <figure>
                            <img src="https://i.postimg.cc/VNrMdRL3/SVM.jpg" alt="SVM Hyperplanes" class="img-fluid">
                            <figcaption class="figc"> Visualisation of LSVM classifier </figcaption>
                        </figure>
                    </div>

                </div>
                <div class="container">
                    <h5 class="sh"><b>1.1 LSVM Parametric Sensitivity Analysis Results: </b></h5>
                    The parameter C was varied to understand sensitivity to the performance. No change was observed
                    from the default value, thus, <b>C</b> value retained at 1.
                    <figure>
                        <img src="https://i.postimg.cc/nhVGcBmJ/LSVM-Sensitivity-Analysis.jpg"
                            alt="LSVM Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity Analysis - for LSVM </figcaption>
                    </figure>
                </div>
            </div>

            <hr id="lr">
            <!-- 2. LR -->
            <div class="algo">
                <h3 class="sh"><b>2. Logistic Regression (LR)</b></h3>
                <div class="row">
                    <div class="col-md-8">
                        <p>
                            In machine learning, this classifier has significant importance and is a go-to algorithm for
                            binary classification tasks. This classifier uses a sigmoid function shown in the figure
                            below which
                            returns a value from 0 to 1 to predict the output. However, multi-class classification tasks
                            are done by applying a one versus all strategy on the classifier.
                            <br> Default parameters that are used in the base case are as follows:
                        <ul>
                            <li>'C': 1.0</li>
                            <li>'class_weight': None</li>
                            <li>'dual': False</li>
                            <li>'fit_intercept': True</li>
                            <li>'intercept_scaling': 1</li>
                            <li>'l1_ratio': None</li>
                            <li>'max_iter': 100</li>
                            <li>'multi_class': 'auto'</li>
                            <li> 'n_jobs': None</li>
                            <li>'penalty': 'l2'</li>
                            <li>'random_state': None</li>
                            <li>'solver': 'lbfgs'</li>
                        </ul>

                        </p>
                    </div>
                    <div class="col-md-4">
                        <figure>
                            <img src="https://i.postimg.cc/tJ0zXc3S/LR.png" alt="LR Sensitivity Results"
                                class="img-fluid">
                            <figcaption class="figc"> Visualisation of LR classifier </figcaption>
                        </figure>
                    </div>

                </div>
                <div class="container">
                    Here 'solver' is the parameter used in optimizing the results. The effect of using different solvers
                    is shown in the Table below.
                    <figure>
                        <img src="https://i.postimg.cc/yd3PCKZ3/LR-Sensitivity-Analysis.jpg"
                            alt="LR solver Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - "solver" for LR </figcaption>
                    </figure>
                </div>
                <div class="container">
                    'Max_iterations' did not give significant change to base parameter, as seen in the figure below.
                    <figure>
                        <img src="https://i.postimg.cc/W4HnyzhP/LR-MI-Sensitivity-Analysis.jpgg"
                            alt="LR MI Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - "max_iterations" for LR </figcaption>
                    </figure>
                </div>
                <div class="container">
                    <figure>
                        <img src="https://i.postimg.cc/QMtmS1Nb/LR-C-Sensitivity-Analysis.jpg"
                            alt="LR C Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - "C" value for LR </figcaption>
                    </figure>
                    From the above analysis, the solver is a change to "liblinear," c value changed from 1 to 10, and no
                    change
                    is made in max_iter.
                </div>
            </div>
            <hr id="bnb">
            <!-- 3. BNB -->
            <div class="algo">
                <h3 class="sh"><b>3. Binomial Naive Bayes (BNB)</b></h3>
                <div>
                    <p>
                        It uses Bayes' theorem with a strong assumption that all the predictors are independent of
                        each other. In simple
                        words, the assumption is that the presence of a feature in a class is independent of any
                        other feature in the same
                        class.

                        <br> Default parameters that are used in the base case are as follows:


                    <ul>
                        <li>'alpha': 1.0</li>
                        <li>'binarize': 0.0</li>
                        <li>'class_prior': None</li>
                        <li>'fit_prior': True</li>
                    </ul>

                    </p>
                </div>
                <div class="container">
                    The following tables give both training data accuracy and testing data accuracy across all the
                    categories.
                    <figure>
                        <img src="https://i.postimg.cc/9FbyS0nN/Sensitivity-analysis-Alpha-BNB.jpg"
                            alt="BNB Alpha Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis- "Alpha" (BNB) </figcaption>
                    </figure>
                </div>
                <div class="container">
                    <figure>
                        <img src="https://i.postimg.cc/0yN7qxg5/Sensitivity-analysis-binarize-BNB.jpg"
                            alt="BNB binarize Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis -"binarize" (BNB) </figcaption>
                    </figure>
                </div>
                <div class="container">
                    <figure>
                        <img src="https://i.postimg.cc/9fKdpknm/Sensitivity-analysis-fit-prior-BNB.jpg"
                            alt="BNB fit_prior Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis-"fit_prior" (BNB)
                        </figcaption>
                    </figure>
                    From the above analysis, the hyperparameters remained the same as the default parameters that
                    are
                    generally used.

                </div>
            </div>
            <hr id="mnb">
            <!-- 4. MNB -->
            <div class="algo">
                <h3 class="sh"><b>4. Multinomial Naive Bayes (MNB)</b></h3>
                <div>
                    <p>
                        It is one of the Naïve Bayes classifiers in which the features are assumed to be drawn from
                        a
                        simple Multinomial
                        distribution. Due to the feature of multi-class prediction, Naïve Bayes classification
                        algorithms are well suited for
                        text classification.


                        <br> Default parameters that are used in the base case are as follows:


                    <ul>
                        <li>'alpha': 1.0</li>
                        <li>'class_prior': None</li>
                        <li>'fit_prior': True</li>
                    </ul>

                    </p>
                </div>
                <div class="container">
                    From the results of the analysis shown below, the alpha value is changed to "0.2".
                    <figure>
                        <img src="https://i.postimg.cc/wMhQ3kF6/MNB-Alpha-Sensitivity-Results.jpg"
                            alt="MNB Alpha Sensitivity Results" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis- "Alpha" (MNB) </figcaption>
                    </figure>
                </div>

            </div>
            <hr id="rf">
            <!-- 5. RF -->
            <div class="algo">
                <h3 class="sh"><b>5. Random Forest (RF)</b></h3>

                <div class="row">
                    <div class="col-md-6">
                        <p>
                            It is made up of a group of decision trees. Random forest algorithm creates decision
                            trees
                            on
                            data samples and then
                            gets the prediction from each of them and finally selects the best solution by means of
                            voting.
                            It is an ensemble method
                            that is better than a single decision tree because it reduces over-fitting by averaging
                            the
                            result.


                            <br> Default parameters that are used in the base case are as follows:

                        <ul>
                            <li>bootstrap=True</li>
                            <li>class_weight=None</li>
                            <li>criterion='gini'</li>
                            <li>max_depth=None</li>
                            <li>max_features='auto'</li>
                            <li>max_leaf_nodes=None</li>
                            <li>min_impurity_decrease=0.0</li>
                            <li>min_impurity_split=None</li>
                            <li>min_samples_leaf=1</li>
                            <li>min_samples_split=2</li>
                            <li>min_weight_fraction_leaf=0.0</li>
                            <li>n_estimators='warn'</li>
                            <li>n_jobs=None</li>
                            <li>random_state=None</li>
                            <li>warm_start=False</li>
                        </ul>

                        </p>
                    </div>
                    <div class="col-md-6">
                        <figure>
                            <img src="RF.jpg" alt="RF Visualization" class="img-fluid">
                            <figcaption class="figc"> Visualisation of Random forest classifier </figcaption>
                        </figure>
                    </div>

                </div>


                <div class="container">
                    Since the number of estimators is mandatory to mention base case considers n_estimators to be
                    100.
                    <br>N_estimators are the number of decision trees used in training the classifier

                    <figure>
                        <img src="https://i.postimg.cc/T3QQrCx1/n-estimators-vs-f1-score-plot.jpg"
                            alt="n_estimators vs f1 score plot" class="img-fluid">
                        <figcaption class="figc"> n_estimators vs f1 score </figcaption>
                    </figure>
                </div>

                <div class="container">
                    Max_features is the number of features to consider when looking for the best split. Here the it
                    is
                    given as the proportion of total number of features and the other three cases ("auto", "sqrt","
                    log2" with inbuilt formulae are also included.
                    <br>Here since max depth is not curtailed, there is a chance of overfitting; hence the parameter
                    is
                    selected only based on F1 score of test data.

                    <figure>
                        <img src="https://i.postimg.cc/2SHFqydB/Sensitivity-analysis-max-features-RF.jpg"
                            alt="Sensitivity analysis - max_features (RF)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - max_features (RF) </figcaption>
                    </figure>
                </div>

                <div class="container">
                    Here since max depth is not curtailed, there is a chance of overfitting; hence the parameter is
                    selected only based on F1 score of test data.
                    Max_depth is the maximum depth of the tree to be considered. If None, then nodes are expanded
                    until all leaves are pure or until all leaves contain less than min_samples_split samples,
                    resulting
                    in overfitting of the classifier.

                    <figure>
                        <img src="https://i.postimg.cc/J0cjRz40/Sensitivity-analysis-max-depth-RF.jpg"
                            alt="Sensitivity analysis -max_depth (RF)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis -max_depth (RF) </figcaption>
                    </figure>
                </div>

                <div class="container">
                    Min_samples_leaf is the minimum number of samples required to be at a leaf node. A split point
                    at
                    any depth will only be considered if it leaves at least min_samples_leaf training samples in
                    each of
                    the left and right branches.

                    <figure>
                        <img src="https://i.postimg.cc/QNpQQ0kD/Sensitivity-analysis-min-samples-leaf-RF.jpg"
                            alt="Sensitivity analysis- min samples leaf (RF)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis- min samples leaf (RF) </figcaption>
                    </figure>
                </div>

                <div class="container">
                    Min_samples_split is the minimum number of samples required to split an internal node. In the
                    following figure, min_samples_split is given in the multiples of 0.01
                    <figure>
                        <img src="Sensitivity analysis - Min_samples_split (RF).jpg"
                            alt="Sensitivity analysis - Min_samples_split (RF)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - Min_samples_split (RF) </figcaption>
                    </figure>
                </div>

                <p>
                    From the above analysis the parameters considered are as follows:
                <ul>
                    <li>Max_depth = 90</li>
                    <li>n_estimators = 100</li>
                    <li>min_samples_split = 0.01</li>
                    <li>min_samples_leaf= 2</li>
                    <li>max_features = auto</li>
                </ul>
                </p>

            </div>

            <hr id="dt">
            <!-- 6. DT -->
            <div class="algo">
                <h3 class="sh"><b>6. Decision Trees (DT)</b></h3>

                <div class="row">
                    <div class="col-md-6">
                        <p>
                            Decision trees can be constructed by an algorithmic approach that can split the dataset
                            differently based on
                            different conditions. Gini index evaluates the binary splits in the dataset and works with
                            the categorical target
                            variable in classification tasks.


                            <br> Default parameters that are used in the base case are as follows:
                        <ul>
                            <li>'ccp_alpha': 0.0</li>
                            <li>'class_weight': None</li>
                            <li>'criterion': 'gini'</li>
                            <li>'max_depth': None</li>
                            <li>'max_features': None</li>
                            <li>'max_leaf_nodes': None</li>
                            <li>'min_impurity_decrease': 0.0</li>
                            <li>'min_impurity_split': None</li>
                            <li>'min_samples_leaf': 1</li>
                            <li>'min_samples_split': 2</li>
                            <li>'min_weight_fraction_leaf': 0.0</li>
                            <li>'random_state': None</li>
                            <li>'splitter': 'best'</li>

                        </ul>

                        </p>
                    </div>
                    <div class="col-md-6">
                        <figure>
                            <img src="https://i.postimg.cc/jdqkz3CL/DT.png" alt="DT Visualization" class="img-fluid">
                            <figcaption class="figc"> Visualisation of Decision tree classifier </figcaption>
                        </figure>
                    </div>
                </div>



                <div class="container">
                    Min_samples_split is the minimum number of samples required to split an internal node.Figure
                    below shows that with the increase in min_samples_split, the training accuracy and testing
                    accuracy have
                    decreased significantly. Here the value with high training accuracy and testing accuracy has
                    been chosen for further
                    analysis.

                    <figure>
                        <img src="https://i.postimg.cc/m2ySs8tj/Sensitivity-analysis-min-samples-split-DT.jpg"
                            alt="Sensitivity analysis - min samples split (DT)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - min samples split (DT)
                        </figcaption>
                    </figure>
                </div>

                <div class="container">
                    <figure>
                        <img src="https://i.postimg.cc/q7zc2rQD/Sensitivity-analysis-min-samples-leaf-DT.jpg"
                            alt="Sensitivity analysis - min_samples_leaf (DT)" class="img-fluid">
                        <figcaption class="figc"> Sensitivity analysis - min_samples_leaf (DT) </figcaption>
                    </figure>
                </div>

                <div class="container">
                    <figure>
                        <img src="https://i.postimg.cc/0QSpHCTp/Sensitivity-analysis-max-depth-DT.jpg"
                            alt="Sensitivity analysis- max_depth (DT)" class="img-fluid">
                        <figcaption class="figc">Sensitivity analysis- max_depth (DT)</figcaption>
                    </figure>
                </div>



                <p>
                    From the above analysis the following parameter are changed:
                <ul>
                    <li>min_samples_leaf = 2</li>
                    <li>min_samples_split = 0.01</li>
                    <li>max_depth = 100</li>
                    <li>max_features = auto</li>
                </ul>
                </p>

            </div>

            <hr id="cnn">
            <!-- 7. CNN -->
            <div class="algo">
                <h3 class="sh"><b>7. Convolutional Neural Network (CNN)</b></h3>
                <div>
                    <p>
                        Convolution can be understood as a sliding function applied to a matrix. The sliding window is
                        called a kernel/filter or feature detector. We choose a n*n filter, multiply its values
                        element-wise with the original matrix, then sum it up. We do this for each element by sliding
                        the filter over the whole matrix to get the whole convolution.
                    </p>
                    <p>
                        CNN's are several layers of convolutions with nonlinear functions like ReLU applied to the
                        results. In CNN, we use convolutions over the input layer to compute the output, resulting in
                        local connections, where each region of input is connected to a neuron in the output.
                    </p>

                    <h5 class="sh"><b>CNN in NLP</b></h5>

                    <p>
                        The input layer for CNN for sentence classification is a sentence/document matrix. Each row of
                        this input layer represents one token, i.e., each row is a vector that represents a word.
                        Typically, we get these word vectors through word embeddings (low-dimensional representations)
                        like word2vec.
                    </p>
                    <p>
                        A key aspect of CNN is pooling later, which is typically applied after the convolution layers.
                        These layers are used to reduce the dimensions of the feature maps. Thus, reducing the number of
                        parameters needed to learn and computation required in the network. One of the most common ways
                        is by using max pooling, i.e., selecting the maximum form of the feature map covered by the
                        filter.
                    </p>
                    <p>
                        Dropout (layer) is a technique used to prevent the model from overfitting. Dropout works by
                        randomly setting the hidden units (neurons that make up hidden layers) to 0 at each update of
                        the training phase.
                    </p>

                    <p>
                        <b>Downsides</b><br>
                        CNNs require the practitioners to specify the exact model architecture i.e set all the
                        accompanying hyperparameters. There are many free parameters in a model, and choosing an
                        appropriate combination of parameters is quite cumbersome. Moreover, exploring and deciding the
                        set of appropriate parameters and their configuration is a computationally costly task. In
                        addition, any CNN model requires specifying the filter size(s), the number of feature
                        maps/filters, the pooling strategy, and the dropout rate.
                    </p>

                    <p>
                        <b>Embedding</b><br>
                        Our model
                        Initialises with an embedding layer (inbuilt Keras library) to convert the text into a vector
                        representation.
                        Word Embedding - Word embedding is a type of dense vector representation used to represent words
                        and documents. It's a step forward from the more conventional bag-of-words encoding schemes,
                        which used huge sparse vectors to represent each word or to score each word inside a vector to
                        represent an entire vocabulary. Since the vocabularies were so massive, each word or document
                        was represented by a significant vector of mostly zero values. Words are instead represented by
                        dense vectors in an embedding, where a vector represents the word's projection into a continuous
                        vector space.
                    </p>

                    <p>
                        A word's position within the vector space is learned from the text and based on the words
                        surrounding the word when used.
                        The position of a word in the learned vector space is referred to as its embedding.
                        Three layers of convolution are performed with filter sizes of 3, 4, and 5, respectively.
                        The number of filters in each layer is 100. The sensitivity analysis is done, and the results
                        verify that 100 is a sufficient number of filters for an excellent F-1 score.
                    </p>

                    <div class="container">
                        <figure>
                            <img src="https://i.postimg.cc/sg07j8yY/Sensitivity-with-respect-to-number-of-filters-CNN.jpg"
                                alt="Sensitivity with respect to number of filters (CNN)" class="img-fluid">
                            <figcaption class="figc">Sensitivity with respect to number of filters (CNN)</figcaption>
                        </figure>
                    </div>

                    <p>
                        The activation function is taken to be "ReLU"
                        The activation function in a neural network is responsible for converting the node's summed
                        weighted input into the node's activation or output for that input. The rectified linear
                        activation function, or ReLU for short, is a piecewise linear function that, if the input is
                        positive, outputs the input directly; otherwise, it outputs zero. It has become the default
                        activation function for many neural networks because a model that uses it is easier to train and
                        often achieves better performance.
                    </p>

                    <p>
                        Max pooling strategy has been used for the model for each convolution. Maximum pooling, or max
                        pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of
                        each feature map.
                        The results are downsampled or pooled feature maps that highlight the most present feature in
                        the patch. Max pooling is done to in part to help over-fitting by providing an abstracted form
                        of the representation. It also reduces the computational cost by reducing the number of
                        parameters to learn and provides basic translation invariance to the internal representation.
                        Max pooling is done by applying a max filter to (usually) non-overlapping subregions of the
                        initial representation.
                    </p>

                    <div class="container">
                        <figure>
                            <img src="https://i.postimg.cc/Qxhp010T/Sensitivity-with-respect-to-Drop-out-rate-CNN.jpg"
                                alt="Sensitivity with respect to Drop-out rate (CNN)" class="img-fluid">
                            <figcaption class="figc">Sensitivity with respect to Drop-out rate (CNN)</figcaption>
                        </figure>
                    </div>

                    <p>
                        The dropout rate is taken to be 0.5, a sensitivity analysis for the dropout has also been done.
                        All the runs are expressed as an average of 100 runs which is quite expensive computationally
                        but ensures the stability of the scores being reported.
                        The sensitivity analysis is a means to justify the parameters been taken and not a means to tune
                        any hyperparameter, as incorporating parameter tuning algorithms like GA with the 100 trial
                        methodology adopted is very expensive computationally.
                    </p>


                </div>
            </div>
            <!--  -->
            <!--  -->


            <br>

            <h4 class="sh toc">Summary</h4>

            The following table summarizes the values of hyperparameters used for various classifiers implemented in
            this
            study.
            <div class="container">
                <figure>
                    <img src="https://i.postimg.cc/3JTZVPJW/Information-on-various-classifiers-used-in-the-current-study.jpg"
                        alt="Information on various classifiers used in the current study" class="img-fluid">
                </figure>
            </div>
        </div>





        </main>

        <footer>
            <div class="container foot">
                <div class="row">
                    <div class="col-3">
                        <a href="index">Home</a>
                    </div>
                    <div class="col-3">
                        <a class="active" href="algorithms">Algorithms</a>
                    </div>
                    <div class="col-3">
                        <a href="demo">Demo</a>
                    </div>
                    <div class="col-3">
                        <a href="contact">Contact</a>
                    </div>
                </div>

            </div>

        </footer>



    </body>


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.5/dist/umd/popper.min.js"
        integrity="sha384-Xe+8cL9oJa6tN/veChSP7q+mnSPaj5Bcu9mPX5F5xIGE0DVittaqT5lorf0EI7Vk"
        crossorigin="anonymous"></script>


</html>